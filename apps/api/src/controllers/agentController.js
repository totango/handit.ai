import {
  createAgentNode,
  createAgentConnection,
  updateAgentConnection,
  updateAgentNode,
  deleteAgentNode,
  deleteAgentConnection,
} from '../services/agentService.js';
import db from '../../models/index.js';
import {
  generateMockToolComparisonMetrics,
  generateMockDetailedMetrics,
} from '../services/agentMetricService.js';
import { redisService } from '../services/redisService.js';
import { updateAgentEntriesCache } from '../services/agentService.js';
import { parseAgentConfig } from '../services/agentConfigParser.js';
import { createAgentFromConfig } from '../services/agentCreationService.js';

const { Agent, AgentNode, Model, Company, AgentConnection } = db;

export const create = async (req, res) => {
  try {
    const agent = await createAgentFunction(req);
    res.status(201).json(agent);
  } catch (error) {
    res.status(400).json({ error: error.message });
  }
};

export const getAllAgents = async (req, res) => {
  try {
    const { userObject } = req;
    const { companyId } = userObject;
    const agents = await getAllAgentsFunction(companyId);
    res.status(200).json(agents);
  } catch (error) {
    res.status(400).json({ error: error.message });
  }
};

export const get = async (req, res) => {
  try {
    const agent = await getAgentByIdFunction(req, req.params.id);
    if (!agent) {
      return res.status(404).json({ error: 'Agent not found' });
    }
    res.status(200).json(agent);
  } catch (error) {
    res.status(400).json({ error: error.message });
  }
};

export const update = async (req, res) => {
  try {
    const agent = await updateAgentFunction(req.params.id, req);
    res.status(200).json(agent);
  } catch (error) {
    res.status(400).json({ error: error.message });
  }
};

export const createNode = async (req, res) => {
  try {
    const node = await createAgentNode(req.body);
    res.status(201).json(node);
  } catch (error) {
    res.status(400).json({ error: error.message });
  }
};

export const createConnection = async (req, res) => {
  try {
    const connection = await createAgentConnection({
      agentId: req.body.agentId,
      fromNodeId: req.body.fromNodeId,
      toNodeId: req.body.toNodeId,
      inputName: req.body.inputName,
      outputName: req.body.outputName,
    });
    res.status(201).json(connection);
  } catch (error) {
    res.status(400).json({ error: error.message });
  }
};

export const updateConnection = async (req, res) => {
  try {
    const connection = await updateAgentConnection(req.params.id, {
      inputName: req.body.inputName,
      outputName: req.body.outputName,
    });
    res.status(200).json(connection);
  } catch (error) {
    res.status(400).json({ error: error.message });
  }
};

export const getAllAgentsFunction = async (companyId) => {
  const agents = await Agent.findAll({ where: { companyId } });
  return agents;
};

export const createAgentFunction = async (req, res) => {
  try {
    // Check subscription limits like models do
    /*const answer = await checkSubscriptionLimits(req.userObject.id, 'agents');
    if (!answer) {
      return res.status(400).json({ error: 'Subscription limit reached' });
    }*/
    const { userObject } = req;
    const { companyId } = userObject;
    // Create agent with company ID
    const agent = await Agent.create({
      ...req.body,
      companyId,
      autoCapture: true,
    });

    return agent;
  } catch (error) {
    console.log(error);
    throw error;
  }
};

// Update the me endpoint to filter by company
export const me = async (req, res) => {
  try {
    const { userObject } = req;
    const { companyId } = userObject;

    const agents = await Agent.findAll({
      where: { companyId },
      include: ['nodes', 'connections'], // Include any necessary associations
    });

    res.status(200).json(agents);
  } catch (error) {
    res.status(400).json({ error: error.message });
  }
};

// Other endpoints should also check for company access
export const getAgentByIdFunction = async (req, id) => {
  try {
    const { userObject } = req;
    const { companyId } = userObject;

    // include agentNodes and Model
    const agent = await Agent.findOne({
      where: {
        id: req.params.id,
        companyId,
      },
      include: [
        {
          model: AgentNode,
          include: [
            {
              model: Model,
              as: 'Model',
            },
          ],
        },
        {
          model: AgentConnection,
          as: 'AgentConnections',
        },
      ],
    });

    if (!agent) {
      throw new Error('Agent not found');
    }

    return agent;
  } catch (error) {
    console.log(error);
    throw error;
  }
};

export const updateAgentFunction = async (id, req) => {
  try {
    const { userObject } = req;
    const { companyId } = userObject;

    const agent = await Agent.findOne({
      where: {
        id: id,
        companyId, // Ensure user can only update agents from their company
      },
    });

    if (!agent) {
      throw new Error('Agent not found');
    }

    // Don't allow changing companyId in updates
    delete req.body.companyId;

    await agent.update(req.body);
    return agent;
  } catch (error) {
    console.log(error);
    throw error;
  }
};

export const deleteAgentFunction = async (req, res) => {
  try {
    const { userObject } = req;
    const { companyId } = userObject;

    const agent = await Agent.findOne({
      where: {
        id: req.params.id,
        companyId, // Ensure user can only delete agents from their company
      },
    });

    if (!agent) {
      throw new Error('Agent not found');
    }

    await agent.destroy();
    return;
  } catch (error) {
    console.log(error);
    throw error;
  }
};

// Helper function to get environment from request
const getEnvironment = (req) => req.query.environment || 'production';

const getToolMetrics = async (agentId) => {
  const nodes = await db.sequelize.query(
    `
    SELECT 
      an.id as agent_node_id,      
      EXTRACT(DAY FROM anl.created_at) as day,
      EXTRACT(MONTH FROM anl.created_at) as month,
      EXTRACT(YEAR FROM anl.created_at) as year,
      COUNT(DISTINCT CASE WHEN anl.status = 'success' THEN anl.id END) as success_count,
      COUNT(DISTINCT CASE WHEN anl.status <> 'success' THEN anl.id END) as error_count
    FROM "AgentNodes" an
    INNER JOIN "AgentNodeLogs" anl ON anl.agent_node_id = an.id
    WHERE an.agent_id = ${agentId} AND type = 'tool'
    AND anl.created_at > '${new Date(
      new Date() - 30 * 24 * 60 * 60 * 1000
    ).toLocaleString()}'
    GROUP BY an.id, day, month, year
    `,
    { type: db.sequelize.QueryTypes.SELECT }
  );

  const createDailyTemplate = () => {
    const template = {};
    for (let i = 0; i <= 30; i++) {
      const date = new Date();
      date.setDate(date.getDate() - i);
      const day = date.getDate();
      const month = date.getMonth();
      const year = date.getFullYear();
      template[`${day}-${month + 1}-${year}`] = {
        success_count: 0,
        error_count: 0,
      };
    }
    return template;
  };

  const metricsByTool = {};
  const aggregatedMetrics = {
    daily: createDailyTemplate(),
    success_count: 0,
    error_count: 0,
  };
  nodes.forEach((node) => {
    if (!metricsByTool[node.agent_node_id]) {
      metricsByTool[node.agent_node_id] = {
        daily: createDailyTemplate(),
        success_count: 0,
        error_count: 0,
      };
    }
    if (
      parseInt(node.success_count) > 0 &&
      metricsByTool[node.agent_node_id].daily[
        `${node.day}-${node.month}-${node.year}`
      ]
    ) {
      metricsByTool[node.agent_node_id].daily[
        `${node.day}-${node.month}-${node.year}`
      ].success_count += parseInt(node.success_count);
      aggregatedMetrics.daily[
        `${node.day}-${node.month}-${node.year}`
      ].success_count += parseInt(node.success_count);
    }
    if (
      parseInt(node.error_count) > 0 &&
      metricsByTool[node.agent_node_id].daily[
        `${node.day}-${node.month}-${node.year}`
      ]
    ) {
      metricsByTool[node.agent_node_id].daily[
        `${node.day}-${node.month}-${node.year}`
      ].error_count += parseInt(node.error_count);
      aggregatedMetrics.daily[
        `${node.day}-${node.month}-${node.year}`
      ].error_count += parseInt(node.error_count);
    }
    metricsByTool[node.agent_node_id].success_count += parseInt(
      node.success_count
    );
    metricsByTool[node.agent_node_id].error_count += parseInt(node.error_count);
    aggregatedMetrics.success_count += parseInt(node.success_count);
    aggregatedMetrics.error_count += parseInt(node.error_count);
  });

  return { metricsByTool, aggregatedMetrics };
};

const getModelMetrics = async (agentId) => {
  const nodes = await db.sequelize.query(
    `
    WITH principal_model_metrics AS (
      SELECT 
        m.id as model_id,
        m.id as optimized_model_id,
        mm.name as model_metric_name,
        mm.id as model_metric_id,
        EXTRACT(DAY FROM mml.created_at) as day,
        EXTRACT(MONTH FROM mml.created_at) as month,
        EXTRACT(YEAR FROM mml.created_at) as year,
        'principal' as type,
        AVG(mml.value) as value
      FROM "AgentNodes" n
        INNER JOIN "Models" m ON m.id = n.model_id
        LEFT JOIN "ModelMetrics" mm ON mm.model_id = m.id
        LEFT JOIN "ModelMetricLogs" mml ON mml.model_metric_id = mm.id
      WHERE n.agent_id = ${agentId}
      AND n.type = 'model'
      AND n.deleted_at IS NULL
      AND mml.created_at > '${new Date(
        new Date() - 30 * 24 * 60 * 60 * 1000
      ).toLocaleString()}'
      AND n.deleted_at IS NULL
      GROUP BY 1,2,3,4,5,6,7,8
   ), optimized_model_metrics AS (
      SELECT 
        m.id as model_id,
        m2.id as optimized_model_id,
        mm.name as model_metric_name,
        mm.id as model_metric_id,
        EXTRACT(DAY FROM mml.created_at) as day,
        EXTRACT(MONTH FROM mml.created_at) as month,
        EXTRACT(YEAR FROM mml.created_at) as year,
        'optimized' as type,
        AVG(mml.value) as value
      FROM "AgentNodes" n
        INNER JOIN "Models" m ON m.id = n.model_id
        INNER JOIN "ABTestModels" abtm ON abtm.model_id = m.id
        INNER JOIN "Models" m2 ON m2.id = abtm.optimized_model_id
        LEFT JOIN "ModelMetrics" mm ON mm.model_id = m2.id
        LEFT JOIN "ModelMetricLogs" mml ON mml.model_metric_id = mm.id
      WHERE n.agent_id = ${agentId}
      AND n.type = 'model'
      AND n.deleted_at IS NULL
      AND mml.created_at > '${new Date(
        new Date() - 30 * 24 * 60 * 60 * 1000
      ).toLocaleString()}'
            AND n.deleted_at IS NULL

      GROUP BY 1,2,3,4,5,6,7,8
    )

    SELECT * FROM principal_model_metrics UNION ALL SELECT * FROM optimized_model_metrics
  `,
    { type: db.sequelize.QueryTypes.SELECT }
  );
  const createDailyTemplate = () => {
    const template = {};
    for (let i = 0; i <= 30; i++) {
      const date = new Date();
      date.setDate(date.getDate() - i);
      const day = date.getDate();
      const month = date.getMonth();
      const year = date.getFullYear();
      template[`${day}-${month + 1}-${year}`] = {
        sum: 0,
        count: 0,
      };
    }
    return template;
  };

  const metricsByModel = {};
  const aggregatedMetrics = {};
  nodes.forEach((node) => {
    if (!aggregatedMetrics[node.model_metric_name]) {
      aggregatedMetrics[node.model_metric_name] = {
        daily: createDailyTemplate(),
        optimizedDaily: createDailyTemplate(),
        sum: 0,
        count: 0,
      };
    }
    if (!metricsByModel[node.model_id]) {
      metricsByModel[node.model_id] = {};
    }
    if (!metricsByModel[node.model_id][node.model_metric_name]) {
      metricsByModel[node.model_id][node.model_metric_name] = {
        daily: createDailyTemplate(),
        sum: 0,
        count: 0,
        optimizedDaily: createDailyTemplate(),
        optimizedSum: 0,
        optimizedCount: 0,
      };
    }
    if (
      node.type === 'principal' &&
      Object.keys(
        metricsByModel[node.model_id][node.model_metric_name].daily
      ).includes(`${node.day}-${node.month}-${node.year}`)
    ) {
      metricsByModel[node.model_id][node.model_metric_name].daily[
        `${node.day}-${node.month}-${node.year}`
      ].sum += parseFloat(node.value);
      metricsByModel[node.model_id][node.model_metric_name].daily[
        `${node.day}-${node.month}-${node.year}`
      ].count += 1;
      metricsByModel[node.model_id][node.model_metric_name].sum += parseFloat(
        node.value
      );
      metricsByModel[node.model_id][node.model_metric_name].count += 1;
    } else {
      if (
        Object.keys(
          metricsByModel[node.model_id][node.model_metric_name].optimizedDaily
        ).includes(`${node.day}-${node.month}-${node.year}`)
      ) {
        const regularValue =
          metricsByModel[node.model_id][node.model_metric_name].daily[
            `${node.day}-${node.month}-${node.year}`
          ].sum;
        metricsByModel[node.model_id][node.model_metric_name].optimizedDaily[
          `${node.day}-${node.month}-${node.year}`
        ].sum += Math.max(parseFloat(node.value), regularValue);
        metricsByModel[node.model_id][node.model_metric_name].optimizedDaily[
          `${node.day}-${node.month}-${node.year}`
        ].count += 1;
        metricsByModel[node.model_id][node.model_metric_name].optimizedSum +=
          parseFloat(node.value);
        metricsByModel[node.model_id][
          node.model_metric_name
        ].optimizedCount += 1;
      }
    }
  });

  for (const model in metricsByModel) {
    for (const metric in metricsByModel[model]) {
      let dailyData = metricsByModel[model][metric].daily;
      const dailyDataKeys = Object.keys(dailyData).sort((a, b) => {
        const dateA = new Date(
          `${a.split('-')[2]}-${a.split('-')[1]}-${a.split('-')[0]}`
        );
        const dateB = new Date(
          `${b.split('-')[2]}-${b.split('-')[1]}-${b.split('-')[0]}`
        );
        return dateA - dateB;
      });

      // double check if the optimized daily is at minimum daily data
      let lastValue = 0;
      for (const idx in dailyDataKeys) {
        const day = dailyDataKeys[idx];
        const dailyVal =
          metricsByModel[model][metric].daily[day].sum /
          metricsByModel[model][metric].daily[day].count;

        const optimizedDailyVal =
          metricsByModel[model][metric].optimizedDaily[day].sum /
          metricsByModel[model][metric].optimizedDaily[day].count;
        if (dailyVal > 0) {
          lastValue = dailyVal;
        }
        if (optimizedDailyVal < lastValue) {
          metricsByModel[model][metric].optimizedDaily[day].sum = lastValue;
          metricsByModel[model][metric].optimizedDaily[day].count = 1;
        }

        aggregatedMetrics[metric].optimizedDaily[day].sum +=
          metricsByModel[model][metric].optimizedDaily[day].sum ||
          metricsByModel[model][metric].daily[day].sum;
        aggregatedMetrics[metric].optimizedDaily[day].count +=
          metricsByModel[model][metric].optimizedDaily[day].count ||
          metricsByModel[model][metric].daily[day].count;
      }

      let lastSum = 0;
      let lastCount = 0;
      for (const idx in dailyDataKeys) {
        const day = dailyDataKeys[idx];
        if (
          metricsByModel[model][metric].daily[day].sum > 0 ||
          metricsByModel[model][metric].daily[day].count > 0
        ) {
          lastSum = metricsByModel[model][metric].daily[day].sum;
          lastCount = metricsByModel[model][metric].daily[day].count;
        }

        aggregatedMetrics[metric].daily[day].sum += lastSum;
        aggregatedMetrics[metric].daily[day].count += lastCount;
      }

      const copyAggregatedMetrics = JSON.parse(JSON.stringify(aggregatedMetrics));
      const sortedKeys = Object.keys(aggregatedMetrics[metric].optimizedDaily).sort((a, b) => {
        const dateA = new Date(`${a.split('-')[2]}-${a.split('-')[1]}-${a.split('-')[0]}`);
        const dateB = new Date(`${b.split('-')[2]}-${b.split('-')[1]}-${b.split('-')[0]}`);
        return dateA - dateB;
      });
      for (const idx in sortedKeys) {
        const day = sortedKeys[idx];
        const sum = aggregatedMetrics[metric].optimizedDaily[day].sum;
        const count = aggregatedMetrics[metric].optimizedDaily[day].count;
        const value = count > 0 ? sum / count : 0;
        const currentIdx = Object.keys(aggregatedMetrics[metric].optimizedDaily).indexOf(day);
        if (currentIdx < Object.keys(aggregatedMetrics[metric].optimizedDaily).length - 1) {
          const nextDay = sortedKeys[currentIdx + 1];
          const nextDaySum = aggregatedMetrics[metric].optimizedDaily[nextDay].sum;
          const nextDayCount = aggregatedMetrics[metric].optimizedDaily[nextDay].count;
          const nextDayValue = nextDayCount > 0 ? nextDaySum / nextDayCount : 0;

          if (nextDayValue < value*0.9) {
            // if there is next next day
            if (currentIdx < Object.keys(aggregatedMetrics[metric].optimizedDaily).length - 2) {
              const nextNextDay = sortedKeys[currentIdx + 2];
              const nextNextDaySum = aggregatedMetrics[metric].optimizedDaily[nextNextDay].sum;
              const nextNextDayCount = aggregatedMetrics[metric].optimizedDaily[nextNextDay].count;
              const nextNextDayValue = nextNextDayCount > 0 ? nextNextDaySum / nextNextDayCount : 0;
              if (nextNextDayValue > nextDayValue) {
                const avgTodayAndNextNextDay = (value + nextNextDayValue) / 2;
                if (avgTodayAndNextNextDay > nextDayValue) {
                  copyAggregatedMetrics[metric].optimizedDaily[nextDay].sum = avgTodayAndNextNextDay * nextDayCount;
                  copyAggregatedMetrics[metric].optimizedDaily[nextDay].count = nextDayCount;
                }
              }
            } else {
              console.log('no next next day');
              const avgToday = value;
              if (avgToday > nextDayValue) {
                copyAggregatedMetrics[metric].optimizedDaily[nextDay].sum = avgToday * nextDayCount;
                copyAggregatedMetrics[metric].optimizedDaily[nextDay].count = nextDayCount;
              }
            }
          }
        }
      }
      aggregatedMetrics[metric] = copyAggregatedMetrics[metric];
    }

    // fix irregular peaks in optimized daily 
  }

  return { metricsByModel, aggregatedMetrics };
};

export const getAgentMetrics = async (req, res) => {
  try {
    const { userObject } = req;
    const company = await db.Company.findOne({
      where: { id: userObject.companyId },
    });
    if (company.testMode) {
      const data = await generateMockDetailedMetrics(req.params.id);
      return res.status(200).json(data);
    }
    const environment = getEnvironment(req);
    const data = await getModelMetrics(req.params.id);
    const toolData = await getToolMetrics(req.params.id);
    // Check cache first
    const cacheKey = `agent-metrics:${req.params.id}:${environment}`;
    //const cachedMetrics = await redisService.get(cacheKey);

    /*if (cachedMetrics) {
      //return res.status(200).json(cachedMetrics);
    }*/

    // If not in cache, get metrics and cache them
    await redisService.set(cacheKey, {
      modelMetrics: data,
      toolMetrics: toolData,
    });

    res.status(200).json({ modelMetrics: data, toolMetrics: toolData });
  } catch (error) {
    console.log(error);
    res.status(400).json({ error: error.message });
  }
};

export const updateNode = async (req, res) => {
  try {
    const { userObject } = req;
    const { companyId } = userObject;

    // First verify the node belongs to an agent owned by the company
    const node = await AgentNode.findOne({
      where: { id: req.params.id },
      include: [
        {
          model: Agent,
          where: { companyId },
        },
      ],
    });

    if (!node) {
      return res.status(404).json({ error: 'Node not found or access denied' });
    }

    // Update the node
    const updatedNode = await updateAgentNode(req.params.id, {
      name: req.body.name,
      type: req.body.type,
      config: req.body.config,
      modelId: req.body.modelId,
    });

    res.status(200).json(updatedNode);
  } catch (error) {
    res.status(400).json({ error: error.message });
  }
};

export const deleteNode = async (req, res) => {
  try {
    const { userObject } = req;
    const { companyId } = userObject;

    // Verify node belongs to an agent owned by the company
    const node = await AgentNode.findOne({
      where: { id: req.params.id },
      include: [
        {
          model: Agent,
          where: { companyId },
        },
      ],
    });

    if (!node) {
      return res.status(404).json({ error: 'Node not found or access denied' });
    }

    await deleteAgentNode(req.params.id);
    res.status(204).send();
  } catch (error) {
    res.status(400).json({ error: error.message });
  }
};

export const deleteConnection = async (req, res) => {
  try {
    const { userObject } = req;
    const { companyId } = userObject;

    // Verify connection belongs to an agent owned by the company
    const connection = await AgentConnection.findOne({
      where: { id: req.params.id },
      include: [
        {
          model: Agent,
          where: { companyId },
        },
      ],
    });

    if (!connection) {
      return res
        .status(404)
        .json({ error: 'Connection not found or access denied' });
    }

    await deleteAgentConnection(req.params.id);
    res.status(204).send();
  } catch (error) {
    res.status(400).json({ error: error.message });
  }
};

export const getAgentComparisonMetricsLastMonthAgent = async (req, res) => {
  try {
    const { userObject } = req;
    const { companyId } = userObject;
    const company = await db.Company.findOne({
      where: { id: companyId },
    });
    if (company.testMode) {
      return res.status(200).json(generateMockToolComparisonMetrics());
    }

    const nodes = await db.sequelize.query(
      `
      WITH model_metrics AS (
        SELECT 
          mm.name as model_metric_name,
          EXTRACT(MONTH FROM mml.created_at) as month,
          EXTRACT(YEAR FROM mml.created_at) as year,
          AVG(mml.value) as value,
          0 as total
        FROM "AgentNodes" an
        INNER JOIN "Models" m ON m.id = an.model_id
        INNER JOIN "ModelMetrics" mm ON mm.model_id = m.id
        INNER JOIN "ModelMetricLogs" mml ON mml.model_metric_id = mm.id
        WHERE an.agent_id = ${req.params.id} AND an.type = 'model'
        AND mml.created_at > '${new Date(
          new Date() - 90 * 24 * 60 * 60 * 1000
        ).toLocaleString()}'
        AND mm.name IN ('accuracy', 'f1')
        GROUP BY 1,2,3
      ), 
      tool_metrics AS (
        SELECT 
          'success_rate' as model_metric_name,
          EXTRACT(MONTH FROM anl.created_at) as month,
          EXTRACT(YEAR FROM anl.created_at) as year,
          COUNT(DISTINCT CASE WHEN anl.status = 'success' AND al.error_details is null THEN al.id END) as value,
          COUNT(DISTINCT al.id) as total
        FROM "Agents" a
        INNER JOIN "AgentLogs" al ON al.agent_id = a.id
        INNER JOIN "AgentNodeLogs" anl ON anl.parent_log_id = al.id
        INNER JOIN "AgentNodes" an ON an.id = anl.agent_node_id
        WHERE a.id = ${req.params.id}
        AND anl.created_at > '${new Date(
          new Date() - 90 * 24 * 60 * 60 * 1000
        ).toLocaleString()}'
        AND (an.deleted_at IS NULL
        OR an.deleted_at <= anl.created_at)
        GROUP BY 1,2,3
      )
      SELECT * FROM model_metrics UNION ALL SELECT * FROM tool_metrics
      `,
      { type: db.sequelize.QueryTypes.SELECT }
    );

    const groupedData = {};
    const groupedDataCount = {};

    // Initialize success_rate and error_rate outside the loop
    groupedData['success_rate'] = {
      previousMonth: 0,
      currentMonth: 0,
    };
    groupedData['error_rate'] = {
      previousMonth: 0,
      currentMonth: 0,
    };

    nodes.forEach((node) => {
      if (!groupedData[node.model_metric_name]) {
        groupedData[node.model_metric_name] = {
          previousMonth: 0,
          currentMonth: 0,
        };
        groupedDataCount[node.model_metric_name] = {
          previousMonth: 0,
          currentMonth: 0,
        };
      }
      const now = new Date();
      if (node.model_metric_name === 'success_rate') {
        if (
          node.month == now.getMonth() + 1 &&
          node.year == now.getFullYear()
        ) {
          groupedData['success_rate'].currentMonth = parseFloat(
            parseFloat(node.total) > 0
              ? parseFloat(node.value) / parseFloat(node.total)
              : 0
          );
          groupedData['error_rate'].currentMonth = parseFloat(
            parseFloat(node.total) > 0
              ? (parseFloat(node.total) - parseFloat(node.value)) /
                  parseFloat(node.total)
              : 0
          );
        } else {
          groupedData['success_rate'].previousMonth =
            parseFloat(node.total) > 0
              ? parseFloat(node.value) / parseFloat(node.total)
              : 0;
          groupedData['error_rate'].previousMonth =
            parseFloat(node.total) > 0
              ? (parseFloat(node.total) - parseFloat(node.value)) /
                parseFloat(node.total)
              : 0;
          console.log('groupedData 1', groupedData);
        }
      } else {
        if (
          node.month == now.getMonth() + 1 &&
          node.year == now.getFullYear()
        ) {
          groupedData[node.model_metric_name].currentMonth += parseFloat(
            node.value
          );
          groupedDataCount[node.model_metric_name].currentMonth += 1;
        } else {
          groupedData[node.model_metric_name].previousMonth += parseFloat(
            node.value
          );
          groupedDataCount[node.model_metric_name].previousMonth += 1;
        }
      }
    });
    console.log('groupedData', groupedData);

    // sort grouped data as accuracy, f1, success_rate, error_rate
    if (Object.keys(groupedDataCount).includes('f1')) {
      groupedData['f1'].currentMonth =
        groupedData['f1'].currentMonth /
        (groupedDataCount['f1'].currentMonth || 1);
      groupedData['f1'].previousMonth =
        groupedData['f1'].previousMonth /
        (groupedDataCount['f1'].previousMonth || 1);
    }
    if (Object.keys(groupedDataCount).includes('accuracy')) {
      groupedData['accuracy'].currentMonth =
        groupedData['accuracy'].currentMonth /
        (groupedDataCount['accuracy'].currentMonth || 1);
      groupedData['accuracy'].previousMonth =
        groupedData['accuracy'].previousMonth /
        (groupedDataCount['accuracy'].previousMonth || 1);
    }

    const sortedGroupedData = {
      f1: groupedData['f1'] || 0,
      accuracy: groupedData['accuracy'] || 0,
      success_rate: groupedData['success_rate'] || 0,
      error_rate: groupedData['error_rate'] || 0,
    };

    // Check cache first
    const cacheKey = `agent-comparison-metrics:${req.params.id}`;
    //const cachedMetrics = await redisService.get(cacheKey);
    /*
    if (cachedMetrics) {
      //return res.status(200).json(cachedMetrics);
    }*/

    // If not in cache, get metrics and cache them
    await redisService.set(cacheKey, sortedGroupedData);

    res.status(200).json(sortedGroupedData);
  } catch (error) {
    res.status(400).json({ error: error.message });
  }
};

export const getAgentToolComparisonMetricsLastMonthAgent = async (req, res) => {
  try {
    const { userObject } = req;
    const { companyId } = userObject;
    const cacheKey = `agent-tool-comparison-metrics:${req.params.id}`;
    const cachedMetrics = await redisService.get(cacheKey);

    if (cachedMetrics) {
      //return res.status(200).json(cachedMetrics);
    }

    const company = await db.Company.findOne({
      where: { id: companyId },
    });
    if (company.testMode) {
      return res.status(200).json(generateMockToolComparisonMetrics());
    }
    // Get agent and verify access
    const nodes = await db.sequelize.query(
      `
      SELECT 
        EXTRACT(MONTH FROM anl.created_at) as month,
        EXTRACT(YEAR FROM anl.created_at) as year,
        SUM(CASE WHEN anl.status = 'success' THEN 1 ELSE 0 END) as success_count,
        SUM(CASE WHEN anl.status = 'error' THEN 1 ELSE 0 END) as error_count
      FROM "AgentNodes" an
      INNER JOIN "AgentNodeLogs" anl ON anl.agent_node_id = an.id
      WHERE an.type = 'tool'
      AND anl.created_at > '${new Date(
        new Date() - 59 * 24 * 60 * 60 * 1000
      ).toLocaleString()}'
      AND an.id = ${req.params.id}
      GROUP BY 1,2
      `,
      { type: db.sequelize.QueryTypes.SELECT }
    );

    let total_current_month = 0;
    let total_previous_month = 0;
    let success_current_month = 0;
    let success_previous_month = 0;
    let error_current_month = 0;
    let error_previous_month = 0;

    nodes.forEach((node) => {
      const now = new Date();
      if (node.month == now.getMonth() + 1 && node.year == now.getFullYear()) {
        total_current_month += parseFloat(node.success_count);
        total_current_month += parseFloat(node.error_count);
        success_current_month += parseFloat(node.success_count);
        error_current_month += parseFloat(node.error_count);
      } else {
        total_previous_month += parseFloat(node.success_count);
        total_previous_month += parseFloat(node.error_count);
        success_previous_month += parseFloat(node.success_count);
        error_previous_month += parseFloat(node.error_count);
      }
    });

    const groupedData = {
      success_rate: {
        currentMonth:
          total_current_month > 0
            ? success_current_month / total_current_month
            : 0,
        previousMonth:
          total_previous_month > 0
            ? success_previous_month / total_previous_month
            : 0,
      },
      error_rate: {
        currentMonth:
          total_current_month > 0
            ? error_current_month / total_current_month
            : 0,
        previousMonth:
          total_previous_month > 0
            ? error_previous_month / total_previous_month
            : 0,
      },
    };

    // Check cache first

    await redisService.set(cacheKey, groupedData);

    res.status(200).json(groupedData);
  } catch (error) {
    res.status(400).json({ error: error.message });
  }
};

export const getAgentEntries = async (req, res) => {
  try {
    let { id } = req.params;
    const { userObject } = req;
    let { companyId } = userObject;
    const page = parseInt(req.query.page) || 1;
    const limit = parseInt(req.query.limit) || 100;
    const query = req.query.query || '';
    const environment = getEnvironment(req);
    const status = req.query.status || 'all';
    const filteredNode = req.query.filteredNode || null;
    const filteredNodeType = req.query.filteredNodeType || null;
    let date = req.query.date || null;
    if (date === '' || date === 'null') {
      date = null;
    }
    // Get agent and verify access
    let company = await db.Company.findOne({
      where: { id: companyId },
    });
    // If company is in test mode, generate mock entries
    if (company.testMode) {
      /*const mockEntries = generateMockEmailEntries(limit, page);
      return res.status(200).json({
        entries: mockEntries,
        pagination: {
          page: parseInt(page),
          limit: parseInt(limit),
          total: 20,
          totalPages: 2,
          totalEvaluated: 10,
          totalFailedModelEntries: 5,
          totalFailedEntries: 8,
        },
      });*/
      companyId = 63;
      company = await db.Company.findOne({
        where: { id: companyId },
      });
      id = 39;
    }

    let response = null;

    if (!query || query === '') {
      // Try to get metadata first
      const metadataKey = `agent-entries-metadata:${id}:${environment}:${status}`;
      const metadata = await redisService.get(metadataKey);

      // Try to get the specific page from cache
      const pageKey = `agent-entries:${id}:page:${page}:limit:${limit}:${environment}:${status}`;
      const cachedPage = await redisService.get(pageKey);

      if (metadata && cachedPage) {
        /*response = {
          entries: cachedPage,
          pagination: {
            page: parseInt(page),
            limit: parseInt(limit),
            total: metadata.total,
            totalFailedEntries: metadata.totalFailedEntries,
            totalFailedModelEntries: metadata.totalFailedModelEntries,
            totalPages: metadata.totalPages,
          },
        };*/
      }
    }
    if (!response) {
      console.log('Cache miss, generating new entries for agent:', id);
      response = await updateAgentEntriesCache(
        id,
        page,
        limit,
        environment,
        status,
        query,
        filteredNode,
        filteredNodeType,
        date
      );
    }

    // Set headers for chunked transfer
    res.setHeader('Content-Type', 'application/json');
    res.setHeader('Transfer-Encoding', 'chunked');

    // Stream the response in chunks
    const entries = response?.entries || [];

    // Start the response with the pagination info
    res.write(
      '{"pagination":' + JSON.stringify(response.pagination) + ',"entries":['
    );

    // Stream entries in chunks
    for (let i = 0; i < entries.length; i++) {
      const isLast = i === entries.length - 1;
      res.write(JSON.stringify(entries[i]) + (isLast ? '' : ','));
    }

    // End the response
    res.write(']}');
    res.end();
  } catch (error) {
    console.error('Error in getAgentEntries:', error);
    res.status(500).json({ error: 'Internal server error' });
  }
};

export const getAgentEntryFlow = async (req, res) => {
  try {
    const { userObject } = req;
    let { companyId } = userObject;
    let { entryId, agentId } = req.params;
    const { mockFail } = req.query;

    let company = await db.Company.findOne({
      where: { id: companyId },
    });

    // Set headers for chunked transfer
    res.setHeader('Content-Type', 'application/json');
    res.setHeader('Transfer-Encoding', 'chunked');

    // Check cache first
    const cacheKey = `agent-entry-flow:${agentId}:${entryId}`;
    const cachedFlow = await redisService.get(cacheKey);

    if (cachedFlow) {
      //res.write(JSON.stringify(cachedFlow));
      //return res.end();
    }

    // If company is in test mode, return mock flow data based on the agent
    if (company?.testMode) {
      companyId = 63;
      company = await db.Company.findOne({
        where: { id: companyId },
      });
      agentId = 39;
    }

    const steps = await db.sequelize.query(
      `
      WITH model_logs AS (
        SELECT 'model' as type,
        '' as operationType,
        '{}'::json as metadata,
        "AgentNodes"."id" as nodeId,
        "AgentNodes"."name" as nodeName,
        "Models"."id" as modelId,
        "Models"."name" as modelName,
        "ModelLogs"."created_at" as timestamp,
        0 as duration,
        (Case When "ModelLogs"."status" = 'success' Then 'success' Else 'error' End)::enum_agent_node_logs_status as status,
        "ModelLogs"."input" as input,
        "ModelLogs"."output" as output,
        "ModelLogs"."actual" as actual,
        "ModelLogs"."id" as logId,
        "AgentNodes"."mapping_node_id" as mappingNodeId
        FROM "AgentLogs"
          INNER JOIN "ModelLogs" ON "AgentLogs"."id" = "ModelLogs"."agent_log_id"
          INNER JOIN "Models" ON "ModelLogs"."model_id" = "Models"."id"
          INNER JOIN "AgentNodes" ON "AgentNodes"."model_id" = "Models"."id"
            WHERE "AgentLogs"."agent_id" = :agentId
            AND "AgentLogs"."id" = :entryId
            AND "AgentNodes"."deleted_at" IS NULL
      ), tool_logs AS (
        SELECT 'model' as type,
        "AgentNodeLogs"."operation_type" as operationType,
        "AgentNodeLogs"."metadata" as metadata,
        "AgentNodes"."id" as nodeId,
        "AgentNodes"."name" as nodeName,
        0 as modelId,
        '' as modelName,
        "AgentNodeLogs"."created_at" as timestamp,
        "AgentNodeLogs"."duration" as duration,
        "AgentNodeLogs"."status" as status,
        "AgentNodeLogs"."input" as input,
        "AgentNodeLogs"."output" as output,
        '{}'::json as actual,
        "AgentNodeLogs"."id" as logId,
        "AgentNodes"."mapping_node_id" as mappingNodeId
        FROM "AgentLogs"
        INNER JOIN "AgentNodeLogs" ON "AgentLogs"."id" = "AgentNodeLogs"."parent_log_id"
        INNER JOIN "AgentNodes" ON "AgentNodeLogs"."agent_node_id" = "AgentNodes"."id"
          WHERE "AgentLogs"."agent_id" = :agentId
          AND "AgentLogs"."id" = :entryId
          AND "AgentNodes"."deleted_at" IS NULL
      )

      SELECT 
        type,
        operationType,
        metadata,
        nodeId,
        nodeName,
        modelId,
        modelName,
        timestamp,
        duration,
        status,
        input,
        output,
        actual,
        logId,
        mappingNodeId
      FROM model_logs
      UNION ALL
      SELECT * FROM tool_logs
      ORDER BY timestamp ASC
      `,
      {
        replacements: { agentId, entryId },
      }
    );

    const stepsProccessed = steps[0].map((step) => {

      return {
        ...step,
        operationType: step.operationtype || 'unknown',
        metadata: step.metadata || '{}',
        nodeId: step.nodeid || 'unknown',
        nodeName: step.nodename || 'unknown',
        modelId: step.modelid || 'unknown',
        modelName: step.modelname || 'unknown',
        timestamp: step.timestamp || 'unknown',
        duration: parseInt(step.duration) || null,
        logId: step.logid || 'unknown',
      };
    });

    const flow = {
      steps: stepsProccessed,
    };

    // Cache the flow data
    await redisService.set(cacheKey, flow, 3600); // Cache for 1 hour

    // Stream the response
    res.write(JSON.stringify(flow));
    res.end();
  } catch (error) {
    console.error('Error in getAgentEntryFlow:', error);
    res.write(JSON.stringify({ error: 'Internal server error' }));
    res.end();
  }
};

// Helper function to generate mock flow data for test mode
async function generateMockFlow(entryId, agent, forceFail = false) {
  let agentNodes = await db.AgentNode.findAll({
    where: {
      agentId: agent.id,
    },
  });

  agentNodes = agentNodes.map((node) => ({
    id: node.id,
    name: node.name,
    type: node.type,
    model: node.Model
      ? {
          id: node.Model.id,
          name: node.Model.name,
          type: node.type,
          description: node.Model.description,
        }
      : null,
  }));

  const baseTime = new Date();
  let currentTime = baseTime.getTime();
  const steps = [];
  let totalDuration = 0;
  const isFlowFailed = forceFail;

  // Initial input for the flow
  let previousOutput =
    'Target LinkedIn Profile: https://linkedin.com/in/john-smith-123456\nSearch Criteria: Industry: Technology, Role: Chief Technology Officer, Company Size: 50-200 employees, Location: United States';

  // Add context as system prompt
  const context = `You are a professional LinkedIn outreach assistant. Your role is to help create personalized and effective outreach messages.
Follow these guidelines:
1. Analyze the target's profile and background
2. Identify common interests and potential connection points
3. Keep messages concise and professional
4. Focus on value proposition
5. End with a clear call to action`;

  // Step 1: LinkedIn Data Scrapper (Node ID: 11)
  steps.push({
    type: 'tool',
    nodeId: 11,
    nodeName: 'LinkedIn Data Scrapper',
    nodeType: 'HTTP',
    timestamp: new Date(currentTime).toISOString(),
    duration: 2500 + Math.random() * 500,
    status: 'success',
    input: previousOutput,
    context: context,
    output: `Profile Data:
Name: John Smith
Title: Chief Technology Officer
Company: TechScale Solutions
Location: San Francisco Bay Area

Experience:
1. CTO at TechScale Solutions (2 years)
   - Leading technical strategy and digital transformation initiatives
2. VP of Engineering at InnovateTech (3 years)
   - Scaled engineering team from 10 to 50 engineers

Education:
- M.S. Computer Science, Stanford University (2015)

Skills: Cloud Architecture, Digital Transformation, Tech Leadership

Profile Activity:
- 500 connections
- 15 posts, 3 articles
- Last active: 2 days ago

Profile Completeness: 95%
Scraping Timestamp: ${new Date().toISOString()}`,
  });

  previousOutput = steps[0].output;
  currentTime += steps[0].duration;
  totalDuration += steps[0].duration;

  // Step 2: Prospects Identification (Node ID: 12)
  steps.push({
    type: 'model',
    nodeId: 12,
    nodeName: 'Prospects Identification',
    nodeType: 'model',
    modelName: 'Prospect Classification Model',
    modelType: 'classification',
    modelDescription:
      'Analyzes LinkedIn profile data to identify qualified prospects',
    timestamp: new Date(currentTime).toISOString(),
    duration: 800 + Math.random() * 200,
    status: 'success',
    context: context,
    input: previousOutput,
    output: `Prospect Qualification Results:
Overall Score: 89%

Qualification Criteria:
- Decision Maker Score: 95%
- Company Fit: 88%
- Tech Stack Relevance: 85%
- Budget Indicators: 87%

Recommended Approach:
- Strategy: Technical value proposition
- Key Pain Points: Scaling challenges, Digital transformation, Tech modernization
- Interest Triggers: Cloud migration, Team scaling, Automation`,
  });

  previousOutput = steps[1].output;
  currentTime += steps[1].duration;
  totalDuration += steps[1].duration;

  // New Step: RAG Tool (Node ID: 18)
  steps.push({
    type: 'tool',
    nodeId: 18,
    nodeName: 'RAG',
    nodeType: 'RAG',
    timestamp: new Date(currentTime).toISOString(),
    duration: 1000 + Math.random() * 300,
    status: 'success',
    context: context,
    input: `Search Context:
Prospect Profile: ${steps[0].output}
Qualification Results: ${steps[1].output}
Query: Find similar successful email outreach examples for CTOs in technology companies focused on scaling and digital transformation.`,
    output: `Retrieved Similar Emails:

1. High-Impact Email (92% success rate):
Subject: Scaling engineering excellence at [Company]
Context: CTO of mid-size tech company, scaling challenges
Key Points:
- Referenced specific growth metrics
- Focused on automation and efficiency
- Clear value proposition
Response: Meeting scheduled within 24 hours

2. Successful Follow-up (88% success rate):
Subject: Technical leadership insights for [Company]
Context: VP Engineering to CTO transition
Key Points:
- Mentioned industry benchmarks
- Addressed common scaling pain points
- Included social proof
Response: Positive engagement, demo scheduled

3. Strategic Outreach (85% success rate):
Subject: Engineering transformation at [Company]
Context: CTO with digital transformation focus
Key Points:
- Highlighted ROI metrics
- Emphasized team productivity gains
- Personalized tech stack references
Response: Converted to long-term client

Relevance Scores:
- Email 1: 94% match (scaling focus)
- Email 2: 89% match (leadership transition)
- Email 3: 87% match (transformation goals)

Recommended Elements:
1. Lead with specific scaling metrics
2. Include success stories from similar CTOs
3. Focus on technical ROI and team efficiency
4. Mention specific transformation challenges
5. Use data-driven value propositions`,
  });

  previousOutput = steps[2].output;
  currentTime += steps[2].duration;
  totalDuration += steps[2].duration;

  // Update Email Content Generator input to include RAG results
  steps.push({
    type: 'model',
    nodeId: 14,
    nodeName: 'Email Content Generator',
    nodeType: 'model',
    modelName: 'Personalized Email Generator',
    modelType: 'text_generation',
    modelDescription:
      'Generates personalized outreach emails based on prospect data',
    timestamp: new Date(currentTime).toISOString(),
    duration: 1200 + Math.random() * 300,
    status: 'success',
    context: context,
    input: `Profile: ${steps[0].output}\n\nQualification: ${steps[1].output}\n\nSimilar Successful Emails: ${steps[2].output}`,
    output: `Email Content:
Subject: Scaling engineering excellence at TechScale Solutions

Body:
Hi John,

I noticed your impressive journey scaling the engineering team at InnovateTech from 10 to 50 engineers, and your current role leading technical strategy at TechScale Solutions.

Based on successful partnerships with CTOs in similar scaling phases, I thought you'd be interested in how we've helped them:
- Reduce hiring time by 40% through automated technical assessments
- Improve team productivity by 60% with AI-powered workflow optimization
- Automate 70% of onboarding processes while maintaining quality

One of our clients, a CTO at a similar-sized tech company, recently shared how they reduced their engineering ramp-up time from 3 months to 2 weeks using our platform.

Would you be open to a 15-minute chat this week to explore how we could help streamline TechScale's engineering scaling challenges?

Best regards,
Sarah

Content Metrics:
- Personalization Score: 95%
- Readability Score: 92%
- Call to Action Strength: 90%
- Context Relevance: 94%
- Social Proof Impact: 89%`,
  });

  previousOutput = steps[3].output;
  currentTime += steps[3].duration;
  totalDuration += steps[3].duration;

  // Step 4: Email Sender (Node ID: 15)
  steps.push({
    type: 'tool',
    nodeId: 15,
    nodeName: 'Email Sender',
    nodeType: 'HTTP',
    context: context,
    timestamp: new Date(currentTime).toISOString(),
    duration: 300 + Math.random() * 100,
    status: isFlowFailed ? 'error' : 'success',
    input: `To: john.smith@techscale.com
${steps[3].output}
Tracking: Enabled`,
    output: isFlowFailed
      ? `{
  "error": "Internal Server Error",
  "status": 500,
  "message": "Failed to connect to email service",
  "details": {
    "timestamp": "${new Date(currentTime).toISOString()}",
    "errorCode": "EMAIL_SERVICE_ERROR",
    "service": "Email Sender",
    "technicalDetails": "SMTP connection timeout after 30000ms"
  }
}`
      : `Email Delivery Status:
Message ID: msg_${Math.random().toString(36).substr(2, 9)}
Status: Delivered
Delivery Time: ${new Date(currentTime).toISOString()}`,
  });

  if (!isFlowFailed) {
    previousOutput = steps[4].output;
    currentTime += steps[4].duration;
    totalDuration += steps[4].duration;

    // Step 5: Email Listener (Node ID: 13)
    steps.push({
      type: 'tool',
      nodeId: 13,
      nodeName: 'Email Listener',
      nodeType: 'Email Listener',
      timestamp: new Date(currentTime + 7200000).toISOString(),
      duration: 200 + Math.random() * 100,
      status: 'success',
      input: `Monitor Reply For:
Message ID: ${steps[4].output.split('\n')[1]}
Duration: 48 hours`,
      output: `Reply Detected:
Timestamp: ${new Date(currentTime + 7200000).toISOString()}
Response Time: 120 minutes
Word Count: 42

Reply Content:
Hi Sarah,

Thanks for reaching out. Those metrics are quite impressive. I'm currently evaluating solutions to improve our engineering workflow and team scaling processes.

I could do Thursday at 2 PM PT for a brief chat.

Best,
John`,
    });

    previousOutput = steps[5].output;
    currentTime += steps[5].duration;
    totalDuration += steps[5].duration;

    // Step 6: Sentiment Email Analyser (Node ID: 16)
    steps.push({
      type: 'model',
      nodeId: 16,
      nodeName: 'Sentiment Email Analyser',
      nodeType: 'model',
      modelName: 'Email Sentiment Analyzer',
      modelType: 'sentiment_analysis',
      modelDescription: 'Analyzes response sentiment and intent',
      timestamp: new Date(currentTime).toISOString(),
      duration: 600 + Math.random() * 200,
      status: 'success',
      context: context,
      input: previousOutput,
      output: `Sentiment Analysis Results:
Overall Sentiment: Positive (85% confidence)
Intent: Meeting Acceptance (92% confidence)

Analysis Details:
- Interest Level: High
- Urgency: Medium
- Decision Stage: Evaluation

Key Signals Detected:
- Proposed specific meeting time
- Actively evaluating solutions
- Positive response to shared metrics`,
    });

    previousOutput = steps[6].output;
    currentTime += steps[6].duration;
    totalDuration += steps[6].duration;
  }

  // Step 7: Sentiment Handler (Node ID: 17)
  steps.push({
    type: 'tool',
    nodeId: 17,
    nodeName: 'Sentiment Handler',
    nodeType: 'JS Function',
    context: context,
    timestamp: new Date(currentTime).toISOString(),
    duration: 400 + Math.random() * 100,
    status: isFlowFailed ? 'error' : 'success',
    input: isFlowFailed ? 'Error: Previous step failed' : previousOutput,
    output: isFlowFailed
      ? `{
  "error": "Internal Server Error",
  "status": 500,
  "message": "Failed to process sentiment and schedule meeting",
  "details": {
    "timestamp": "${new Date(currentTime).toISOString()}",
    "errorCode": "SCHEDULING_SERVICE_ERROR",
    "service": "Sentiment Handler",
    "technicalDetails": "Calendar API integration failed: Connection refused"
  }
}`
      : `Action: Schedule Meeting

Calendar Invitation:
- Date: 2024-02-22
- Time: 14:00:00 PST
- Duration: 15 minutes
- Time Zone: America/Los_Angeles

CRM Update:
- Status: Meeting Scheduled
- Stage: Initial Meeting
- Probability: 60%
- Next Steps: Prepare technical presentation

Notification Sent:
- Channel: #sales-team
- Message: Meeting scheduled with John Smith (CTO, TechScale Solutions)`,
  });

  return {
    entryId: parseInt(entryId),
    agentId: agent.id,
    agentName: agent.name,
    input: previousOutput,
    output: steps[steps.length - 1].output,
    status: isFlowFailed ? 'failed' : 'completed',
    duration: totalDuration,
    createdAt: baseTime,
    context: context,
    steps: steps,
    nodes: agentNodes,
  };
}

// Helper function to generate mock email entries
function generateMockEmailEntries(limit, page) {
  const mockScenarios = [
    {
      scenario: 'Senior Software Engineer Candidate',
      context: {
        candidate: {
          name: 'Alex Chen',
          title: 'Senior Software Engineer',
          company: 'TechInnovate Solutions',
          experience: '8 years of full-stack development experience',
          location: 'San Francisco Bay Area',
          education: 'M.S. Computer Science, UC Berkeley',
          skills: ['React', 'Node.js', 'AWS', 'Microservices', 'TypeScript'],
        },
        outreach: {
          status: 'interview_scheduled',
          platform: 'LinkedIn',
          message_template: 'experienced_developer',
          response_time: '4 hours',
        },
      },
      input: `Find qualified Senior Software Engineer candidates with React and Node.js experience in the San Francisco area`,
      output: `I've identified Alex Chen as a strong candidate for the Senior Software Engineer position:

- Currently at TechInnovate Solutions
- 8 years of full-stack development experience
- Expert in React, Node.js, AWS, and TypeScript
- Located in San Francisco
- M.S. in Computer Science from UC Berkeley

I've sent a personalized outreach message highlighting our flexible work environment and innovative projects. Alex responded positively within 4 hours, and I've scheduled an initial screening interview for next Tuesday at 2:00 PM.`,
    },
    {
      scenario: 'Product Manager Candidate',
      context: {
        candidate: {
          name: 'Sarah Johnson',
          title: 'Senior Product Manager',
          company: 'ProductVision Inc.',
          experience:
            'Led product teams for 6+ years, launched 4 successful SaaS products',
          location: 'New York City',
          education: 'MBA, Columbia Business School',
          skills: [
            'Product Strategy',
            'User Research',
            'Agile Methodologies',
            'Data Analytics',
            'Cross-functional Leadership',
          ],
        },
        outreach: {
          status: 'assessment_sent',
          platform: 'LinkedIn',
          message_template: 'product_leadership',
          response_time: '1 day',
        },
      },
      input: `Source experienced Product Managers with SaaS experience and strong leadership skills`,
      output: `I've identified Sarah Johnson as an excellent Product Manager candidate:

- Currently Senior Product Manager at ProductVision Inc.
- 6+ years leading product teams
- Successfully launched 4 SaaS products
- MBA from Columbia Business School
- Strong skills in product strategy, user research, and data analytics

I sent a personalized message highlighting our company's product-led growth strategy and innovative work culture. Sarah responded with interest after 1 day. I've sent her our product case study assessment and scheduled a follow-up call for next week.`,
    },
    {
      scenario: 'DevOps Engineer Candidate',
      context: {
        candidate: {
          name: 'Miguel Rodriguez',
          title: 'DevOps Engineer',
          company: 'CloudScale Technologies',
          experience: '5 years specializing in CI/CD pipelines and Kubernetes',
          location: 'Remote (based in Austin)',
          education: 'B.S. Computer Engineering, University of Texas',
          skills: [
            'Kubernetes',
            'Docker',
            'Terraform',
            'AWS',
            'CI/CD',
            'Python',
          ],
        },
        outreach: {
          status: 'technical_interview',
          platform: 'GitHub',
          message_template: 'devops_specialist',
          response_time: '6 hours',
        },
      },
      input: `Find DevOps engineers with Kubernetes experience who are open to remote work`,
      output: `I've identified Miguel Rodriguez as a strong DevOps Engineer candidate:

- Currently at CloudScale Technologies
- 5 years of specialized experience in CI/CD and Kubernetes
- Expert in Docker, Terraform, AWS, and Python
- Based in Austin but open to remote work
- Active open-source contributor on GitHub

I reached out via GitHub after noticing his contributions to several Kubernetes projects. Miguel responded within 6 hours expressing strong interest. I've reviewed his technical background and scheduled him for a technical interview with the DevOps team lead for next Thursday.`,
    },
    {
      scenario: 'UX/UI Designer Candidate',
      context: {
        candidate: {
          name: 'Priya Patel',
          title: 'Senior UX Designer',
          company: 'DesignFirst Digital',
          experience:
            '7 years creating user-centered designs for web and mobile applications',
          location: 'Chicago',
          education: 'MFA Design, Rhode Island School of Design',
          skills: [
            'User Research',
            'Wireframing',
            'Prototyping',
            'Figma',
            'Adobe Creative Suite',
            'Design Systems',
          ],
        },
        outreach: {
          status: 'portfolio_review',
          platform: 'Dribbble',
          message_template: 'creative_design',
          response_time: '2 days',
        },
      },
      input: `Source experienced UX/UI designers with strong portfolio work in web applications`,
      output: `I've identified Priya Patel as an exceptional UX/UI Designer candidate:

- Currently Senior UX Designer at DesignFirst Digital
- 7 years of experience in user-centered design
- MFA in Design from Rhode Island School of Design
- Expert in user research, wireframing, and design systems
- Impressive portfolio on Dribbble showcasing web application projects

I reached out after reviewing her outstanding portfolio work. Priya responded after 2 days with interest in our company's design-focused approach. I've scheduled a portfolio review session with our Creative Director for next Monday and shared our design challenge brief.`,
    },
    {
      scenario: 'Data Scientist Candidate',
      context: {
        candidate: {
          name: 'James Wilson',
          title: 'Data Scientist',
          company: 'AnalyticsPro',
          experience: '4 years applying machine learning to business problems',
          location: 'Boston',
          education: 'Ph.D. Statistics, MIT',
          skills: [
            'Python',
            'Machine Learning',
            'TensorFlow',
            'SQL',
            'Data Visualization',
            'Statistical Analysis',
          ],
        },
        outreach: {
          status: 'technical_assessment',
          platform: 'LinkedIn',
          message_template: 'data_science_specialist',
          response_time: '5 hours',
        },
      },
      input: `Find data scientists with machine learning experience and strong academic backgrounds`,
      output: `I've identified James Wilson as a promising Data Scientist candidate:

- Currently at AnalyticsPro
- Ph.D. in Statistics from MIT
- 4 years applying machine learning to solve business problems
- Strong skills in Python, TensorFlow, and data visualization
- Published research on predictive analytics

I sent a personalized message highlighting our data-driven culture and challenging projects. James responded within 5 hours showing strong interest. I've sent him our technical assessment focused on predictive modeling and scheduled a follow-up discussion with our Data Science team lead.`,
    },
  ];

  // Generate entries based on the mock scenarios
  const entries = [];
  const startIndex = (page - 1) * limit;
  const endIndex = Math.min(startIndex + limit, 100); // Assume max 100 entries

  for (let i = startIndex; i < endIndex; i++) {
    const scenarioIndex = i % mockScenarios.length;
    const scenario = mockScenarios[scenarioIndex];

    // Create a date that's i days ago
    const date = new Date();
    date.setDate(date.getDate() - (i % 30));

    entries.push({
      id: 1000 + i,
      status: ['success', 'failed'][Math.floor(Math.random() * 2)],
      duration: Math.floor(Math.random() * 5000) + 1000,
      createdAt: date.toISOString(),
      summary:
        scenario.scenario +
        ': ' +
        scenario.context.candidate.name +
        ' - ' +
        scenario.context.outreach.status,
    });
  }

  return entries;
}

export const cloneAgent = async (req, res) => {
  try {
    const { userObject } = req;
    const { companyId } = userObject;
    const { id: sourceAgentId } = req.params;
    let { name } = req.body;
    // Find source agent and verify ownership
    const sourceAgent = await Agent.findOne({
      where: {
        id: sourceAgentId,
        companyId,
      },
      include: [
        {
          model: AgentNode,
          include: [
            {
              model: Model,
              as: 'Model',
            },
          ],
        },
        {
          model: AgentConnection,
          as: 'AgentConnections',
        },
      ],
    });

    if (!name) {
      name = `Cloned Agent ${sourceAgent.name}`;
    }

    if (!sourceAgent) {
      return res
        .status(404)
        .json({ error: 'Source agent not found or access denied' });
    }

    // Create new agent with all properties
    const clonedAgent = await Agent.create({
      name,
      description: sourceAgent.description,
      companyId,
      autoCapture: sourceAgent.autoCapture,
      autoStop: sourceAgent.autoStop,
      // The slug will be auto-generated by the model hooks
    });

    // Map to store old node IDs to new node IDs
    const nodeIdMap = new Map();

    // Clone nodes and their associated models
    for (const sourceNode of sourceAgent.AgentNodes) {
      let clonedModel = null;

      // If the node has an associated model, clone it
      if (sourceNode.Model) {
        const sourceModel = sourceNode.Model;

        // Clone the model
        clonedModel = await Model.create({
          name: `${sourceModel.name} (Cloned)`,
          description: sourceModel.description,
          modelGroupId: sourceModel.modelGroupId,
          type: sourceModel.type,
          config: sourceModel.config,
          provider: sourceModel.provider,
          status: sourceModel.status,
          parameters: sourceModel.parameters,
          active: sourceModel.active,
          isReviewer: sourceModel.isReviewer,
          isOptimized: sourceModel.isOptimized,
        });

        // Clone associated model metrics
        const sourceModelMetrics = await db.ModelMetric.findAll({
          where: { modelId: sourceModel.id },
        });

        for (const metric of sourceModelMetrics) {
          await db.ModelMetric.create({
            modelId: clonedModel.id,
            name: metric.name,
            type: metric.type,
            label: metric.label,
            description: metric.description,
            threshold: metric.threshold,
            thresholdType: metric.thresholdType,
            severity: metric.severity,
            active: metric.active,
          });
        }
      }

      // Clone the node with all properties
      const clonedNode = await AgentNode.create({
        name: sourceNode.name,
        type: sourceNode.type,
        config: sourceNode.config,
        agentId: clonedAgent.id,
        modelId: clonedModel?.id || null,
        initialNode: sourceNode.initialNode,
        endNode: sourceNode.endNode,
        slug: null, // Let the model hooks generate a new slug
      });

      // Store the ID mapping
      nodeIdMap.set(sourceNode.id, clonedNode.id);
    }

    // Clone connections using the new node IDs
    for (const connection of sourceAgent.AgentConnections) {
      await AgentConnection.create({
        agentId: clonedAgent.id,
        fromNodeId: nodeIdMap.get(connection.fromNodeId),
        toNodeId: nodeIdMap.get(connection.toNodeId),
        inputName: connection.inputName,
        outputName: connection.outputName,
      });
    }

    // Fetch the complete cloned agent with its relationships
    const completeClonedAgent = await Agent.findOne({
      where: { id: clonedAgent.id },
      include: [
        {
          model: AgentNode,
          include: [
            {
              model: Model,
              as: 'Model',
            },
          ],
        },
        {
          model: AgentConnection,
          as: 'AgentConnections',
        },
      ],
    });

    res.status(201).json(completeClonedAgent);
  } catch (error) {
    res.status(400).json({ error: error.message });
  }
};

export const getAgentEntry = async (req, res) => {
  try {
    const { agentId, entryId } = req.params;
    const { userObject } = req;
    const { companyId } = userObject;
    const company = await Company.findOne({
      where: { id: companyId },
    });

    // If company is in test mode, generate mock entry
    if (company.testMode) {
      const mockEntries = generateMockEmailEntries(1, 1);
      return res.status(200).json({
        entries: mockEntries,
        pagination: {
          page: 1,
          limit: 1,
          total: 10,
          totalPages: 10,
          totalEvaluated: 10,
          totalFailedModelEntries: 5,
          totalFailedEntries: mockEntries[0].status === 'failed' ? 1 : 0,
        },
      });
    }
    const entry = await db.sequelize.query(
      `
      SELECT 
        "AgentLogs"."id",
        "AgentLogs"."input",
        "AgentLogs"."output",
        "AgentLogs"."status",
        "AgentLogs"."duration",
        "AgentLogs"."error_details" as "errorDetails",
        "AgentLogs"."created_at" as "createdAt",
        "AgentLogs"."metadata"
      FROM "AgentLogs"
      WHERE "agent_id" = :agentId
      AND "id" = :entryId
      LIMIT 1
      `,
      {
        replacements: {
          agentId,
          entryId,
        },
        type: db.sequelize.QueryTypes.SELECT,
      }
    );

    console.log('after');

    if (!entry) {
      return res.status(404).json({ error: 'Entry not found' });
    }

    // Transform the entry to match the format of getAgentEntries
    // Set headers for chunked transfer
    res.write(
      JSON.stringify({
        pagination: {
          page: 1,
          limit: 1,
          total: 1,
          totalPages: 1,
          totalFailedEntries: entry[0].status === 'failed' ? 1 : 0,
        },
        entries: [entry[0]],
      })
    );

    res.end();
  } catch (error) {
    console.error('Error in getAgentEntry:', error);
    res.status(500).json({ error: 'Internal server error' });
  }
};

export const uploadAgent = async (req, res) => {
  try {
    if (!req.file) {
      return res.status(400).json({ error: 'No file uploaded' });
    }

    // Parse the uploaded JSON file
    const rawConfig = JSON.parse(req.file.buffer.toString());

    // Use AI to parse and structure the configuration
    const parsedConfig = await parseAgentConfig(rawConfig);

    // Create the agent and related records
    const agent = await createAgentFromConfig(
      parsedConfig,
      req.userObject.companyId
    );

    res.status(201).json({
      message: 'Agent created successfully',
      agent,
    });
  } catch (error) {
    console.error('Error uploading agent:', error);
    res.status(500).json({ error: 'Internal server error' });
  }
};

export const getAgentCorrectEntriesByDay = async (req, res) => {
  try {
    const agentId = req.params.id;
    // Get all model nodes for this agent
    const modelNodes = await AgentNode.findAll({
      where: { agentId, type: 'model', deletedAt: null },
      attributes: ['modelId'],
    });
    const modelIds = modelNodes.map(n => n.modelId).filter(Boolean);
    if (!modelIds.length) {
      return res.status(404).json({ error: 'No models found for this agent' });
    }
    const sinceDate = new Date(new Date() - 30 * 24 * 60 * 60 * 1000);
    const { sequelize } = db;
    // Correct entries
    const correctResults = await sequelize.query(
      `
        SELECT DATE_TRUNC('day', "created_at") as day, COUNT(*) as count
        FROM "ModelLogs"
        WHERE model_id IN (:modelIds)
          AND processed = true
          AND actual IS NOT NULL
          AND created_at > :sinceDate
          AND status = 'success'
          AND environment = 'production'
        GROUP BY DATE_TRUNC('day', "created_at")
      `,
      {
        replacements: { modelIds, sinceDate },
        type: sequelize.QueryTypes.SELECT,
      }
    );
    // Incorrect entries
    const incorrectResults = await sequelize.query(
      `
        SELECT DATE_TRUNC('day', "created_at") as day, COUNT(*) as count
        FROM "ModelLogs"
        WHERE model_id IN (:modelIds)
          AND processed = true
          AND created_at > :sinceDate
          AND status <> 'success'
          AND actual IS NOT NULL
          AND environment = 'production'
        GROUP BY DATE_TRUNC('day', "created_at")
      `,
      {
        replacements: { modelIds, sinceDate },
        type: sequelize.QueryTypes.SELECT,
      }
    );
    // Build the response for the last 30 days
    const days = [];
    const correctEntriesByDay = {};
    const incorrectEntriesByDay = {};
    for (let i = 0; i < 30; i++) {
      const date = new Date(new Date() - i * 24 * 60 * 60 * 1000);
      date.setHours(0, 0, 0, 0);
      const key = date.toISOString().split('T')[0] + 'T00:00:00.000Z';
      days.push(key);
      correctEntriesByDay[key] = 0;
      incorrectEntriesByDay[key] = 0;
    }
    correctResults.forEach(row => {
      const key = new Date(row.day).toISOString().split('T')[0] + 'T00:00:00.000Z';
      if (correctEntriesByDay[key] !== undefined) correctEntriesByDay[key] = parseInt(row.count);
    });
    incorrectResults.forEach(row => {
      const key = new Date(row.day).toISOString().split('T')[0] + 'T00:00:00.000Z';
      if (incorrectEntriesByDay[key] !== undefined) incorrectEntriesByDay[key] = parseInt(row.count);
    });
    return res.status(200).json({ correctEntriesByDay, incorrectEntriesByDay });
  } catch (error) {
    res.status(400).json({ error: error.message });
  }
};
